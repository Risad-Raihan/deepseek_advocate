"""
Legal Response Generator for Bengali Legal Advocate
Optimized for Local LM Studio + DeepSeek Integration
"""

import logging
import json
from typing import Dict, List, Optional, Any
import requests
from datetime import datetime
import re

class BengaliLegalResponseGenerator:
    """Advanced response generator for Bengali legal queries using local LM Studio"""
    
    def __init__(self, lm_studio_url: str = "http://localhost:1234/v1", model_name: str = "deepseek"):
        self.lm_studio_url = lm_studio_url
        self.model_name = model_name
        self.setup_logging()
        
        # Response generation parameters
        self.max_response_length = 1500
        self.temperature = 0.3  # Low temperature for legal accuracy
        self.top_p = 0.9
        
        # Legal response templates
        self.response_templates = self._initialize_response_templates()
        
        # Verification patterns for Bengali legal content
        self.verification_patterns = self._initialize_verification_patterns()
    
    def setup_logging(self):
        """Setup logging for response generator"""
        self.logger = logging.getLogger(__name__)
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
    
    def _initialize_response_templates(self) -> Dict[str, str]:
        """Initialize Bengali legal response templates for different domains"""
        return {
            'family_law': """
ржЖржкржирж╛рж░ ржкрж╛рж░рж┐ржмрж╛рж░рж┐ржХ ржЖржЗржи рж╕ржВржХрзНрж░рж╛ржирзНржд ржкрзНрж░рж╢рзНржирзЗрж░ ржЙрждрзНрждрж░:

{main_response}

ржЖржЗржирж┐ ржнрж┐рждрзНрждрж┐:
{legal_basis}

ржкрзНрж░ржпрж╝рзЛржЬржирзАржпрж╝ ржкржжржХрзНрж╖рзЗржк:
{required_steps}

рж╕рждрж░рзНржХрждрж╛: {warning}
""",
            
            'property_law': """
рж╕ржорзНржкрждрзНрждрж┐ ржЖржЗржи рж╕ржВржХрзНрж░рж╛ржирзНржд ржЖржкржирж╛рж░ ржкрзНрж░рж╢рзНржирзЗрж░ ржЙрждрзНрждрж░:

{main_response}

ржкрзНрж░рж╛рж╕ржЩрзНржЧрж┐ржХ ржЖржЗржирж┐ ржмрж┐ржзрж╛ржи:
{legal_provisions}

ржкрзНрж░ржпрж╝рзЛржЬржирзАржпрж╝ ржХрж╛ржЧржЬржкрждрзНрж░:
{required_documents}

ржкрж░рж╛ржорж░рзНрж╢: {advice}
""",
            
            'constitutional_law': """
рж╕рж╛ржВржмрж┐ржзрж╛ржирж┐ржХ ржЖржЗржи рж╕ржВржХрзНрж░рж╛ржирзНржд ржЙрждрзНрждрж░:

{main_response}

рж╕ржВржмрж┐ржзрж╛ржирзЗрж░ ржкрзНрж░рж╛рж╕ржЩрзНржЧрж┐ржХ ржЕржирзБржЪрзНржЫрзЗржж:
{constitutional_articles}

ржорзМрж▓рж┐ржХ ржЕржзрж┐ржХрж╛рж░ рж╕ржВржХрзНрж░рж╛ржирзНржд рждржерзНржп:
{fundamental_rights}

ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржирзЛржЯ: {important_note}
""",
            
            'court_procedure': """
ржЖржжрж╛рж▓рждрж┐ ржкрзНрж░ржХрзНрж░рж┐ржпрж╝рж╛ рж╕ржВржХрзНрж░рж╛ржирзНржд ржирж┐рж░рзНржжрзЗрж╢ржирж╛:

{main_response}

ржкрзНрж░ржпрж╝рзЛржЬржирзАржпрж╝ ржкрзНрж░ржХрзНрж░рж┐ржпрж╝рж╛:
{required_procedure}

рж╕ржоржпрж╝рж╕рзАржорж╛:
{time_limits}

ржЖржжрж╛рж▓ржд ржлрж┐ ржУ ржЦрж░ржЪ:
{court_fees}

ржкрж░ржмрж░рзНрждрзА ржкржжржХрзНрж╖рзЗржк: {next_steps}
""",
            
            'general': """
ржЖржкржирж╛рж░ ржЖржЗржирж┐ ржкрзНрж░рж╢рзНржирзЗрж░ ржЙрждрзНрждрж░:

{main_response}

ржкрзНрж░рж╛рж╕ржЩрзНржЧрж┐ржХ ржЖржЗржирж┐ рждржерзНржп:
{legal_information}

ржкрж░рж╛ржорж░рзНрж╢:
{recommendations}

ржжрзНрж░рж╖рзНржЯржмрзНржп: {disclaimer}
"""
        }
    
    def _initialize_verification_patterns(self) -> Dict[str, List[str]]:
        """Initialize patterns for verifying Bengali legal content accuracy"""
        return {
            'legal_terms': [
                r'ржзрж╛рж░рж╛\s*\d+',
                r'ржЕржирзБржЪрзНржЫрзЗржж\s*\d+',
                r'\d{4}\s*рж╕рж╛рж▓рзЗрж░.*ржЖржЗржи',
                r'ржЖржжрж╛рж▓ржд',
                r'ржмрж┐ржЪрж╛рж░ржХ',
                r'ржорж╛ржорж▓рж╛',
                r'ржЖржкрж┐рж▓'
            ],
            'legal_procedures': [
                r'ржЖржмрзЗржжржи',
                r'ржирзЛржЯрж┐рж╢',
                r'рж╢рзБржирж╛ржирж┐',
                r'ржкрзНрж░ржорж╛ржг',
                r'рж╕рж╛ржХрзНрж╖рзА',
                r'рж░рж╛ржпрж╝'
            ],
            'family_law_terms': [
                r'рждрж╛рж▓рж╛ржХ',
                r'ржЦрзЛрж░ржкрзЛрж╢',
                r'ржжрзЗржиржорзЛрж╣рж░',
                r'ржЙрждрзНрждрж░рж╛ржзрж┐ржХрж╛рж░',
                r'ржЕржнрж┐ржнрж╛ржмржХрждрзНржм'
            ]
        }
    
    def generate_comprehensive_legal_response(self, rag_output: Dict) -> Dict[str, Any]:
        """
        Generate comprehensive legal response using local LM Studio
        
        Args:
            rag_output: Complete RAG system output with context and citations
            
        Returns:
            Comprehensive legal response with metadata
        """
        try:
            self.logger.info("Generating comprehensive legal response...")
            
            # Build prompt for LM Studio
            system_prompt = self._build_system_prompt()
            user_prompt = self._build_user_prompt(rag_output)
            
            # Generate response using local LM Studio
            generated_response = self._call_lm_studio(system_prompt, user_prompt)
            
            if not generated_response:
                # Fallback to template-based response
                generated_response = self._generate_template_response(rag_output)
            
            # Post-process and verify response
            processed_response = self._post_process_response(generated_response, rag_output)
            
            # Format final response
            final_response = self._format_final_response(processed_response, rag_output)
            
            self.logger.info("Legal response generated successfully")
            return final_response
            
        except Exception as e:
            self.logger.error(f"Error generating legal response: {e}")
            return self._generate_error_response(str(e), rag_output)
    
    def _build_system_prompt(self) -> str:
        """Build system prompt for Bengali legal expert"""
        return """ржЖржкржирж┐ ржПржХржЬржи ржмрж┐рж╢рзЗрж╖ржЬрзНржЮ ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рж┐ ржЖржЗржиржЬрзАржмрзА ржПржмржВ ржЖржЗржирж┐ ржкрж░рж╛ржорж░рзНрж╢ржжрж╛рждрж╛ред ржЖржкржирж╛рж░ ржжрж╛ржпрж╝рж┐рждрзНржм:

1. ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛ржпрж╝ рж╕рзБрж╕рзНржкрж╖рзНржЯ ржПржмржВ ржирж┐рж░рзНржнрзБрж▓ ржЖржЗржирж┐ ржкрж░рж╛ржорж░рзНрж╢ ржкрзНрж░ржжрж╛ржи ржХрж░рж╛
2. ржкрзНрж░рж╛рж╕ржЩрзНржЧрж┐ржХ ржЖржЗржи, ржзрж╛рж░рж╛, ржПржмржВ ржЕржирзБржЪрзНржЫрзЗржжрзЗрж░ рж╕ржарж┐ржХ ржЙрж▓рзНрж▓рзЗржЦ ржХрж░рж╛
3. ржмрзНржпржмрж╣рж╛рж░рж┐ржХ ржкржжржХрзНрж╖рзЗржк ржПржмржВ ржкрж░рж╛ржорж░рзНрж╢ ржжрзЗржУржпрж╝рж╛
4. ржЖржЗржирж┐ ржЬржЯрж┐рж▓рждрж╛ рж╕рж╣ржЬ ржнрж╛рж╖рж╛ржпрж╝ ржмрзНржпрж╛ржЦрзНржпрж╛ ржХрж░рж╛
5. рж╕рждрж░рзНржХрждрж╛ржорзВрж▓ржХ рждржерзНржп ржПржмржВ ржжрж╛ржмрж┐рждрзНржпрж╛ржЧ ржЕржирзНрждрж░рзНржнрзБржХрзНржд ржХрж░рж╛

ржирж┐ржпрж╝ржо:
- рж╢рзБржзрзБржорж╛рждрзНрж░ ржкрзНрж░ржжрждрзНржд ржкрзНрж░рж╕ржЩрзНржЧрзЗрж░ ржнрж┐рждрзНрждрж┐рждрзЗ ржЙрждрзНрждрж░ ржжрж┐ржи
- ржЕржирзБржорж╛ржи ржмрж╛ ржнрзБрж▓ рждржерзНржп ржжрж┐ржмрзЗржи ржирж╛
- ржЖржЗржирж┐ ржЙрзОрж╕рзЗрж░ рж╕ржарж┐ржХ рж░рзЗржлрж╛рж░рзЗржирзНрж╕ ржжрж┐ржи
- ржмрзНржпржмрж╣рж╛рж░рж┐ржХ ржПржмржВ ржХрж╛рж░рзНржпржХрж░ ржкрж░рж╛ржорж░рзНрж╢ ржжрж┐ржи
- ржкрзЗрж╢рж╛ржжрж╛рж░ ржПржмржВ рж╕ржорзНржорж╛ржиржЬржиржХ ржнрж╛рж╖рж╛ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржи"""
    
    def _build_user_prompt(self, rag_output: Dict) -> str:
        """Build user prompt with context and query"""
        try:
            query = rag_output.get('query_analysis', {}).get('original_query', '')
            context = rag_output.get('response_context', '')
            domain = rag_output.get('legal_domain', 'general')
            
            prompt = f"""
ржкрзНрж░рж╢рзНржи: {query}

ржЖржЗржирж┐ ржкрзНрж░рж╕ржЩрзНржЧ ржПржмржВ рждржерзНржп:
{context}

ржЖржЗржирж┐ ржХрзНрж╖рзЗрждрзНрж░: {domain}

ржЕржирзБржЧрзНрж░рж╣ ржХрж░рзЗ ржЙржкрж░рзЗрж░ ржкрзНрж░рж╕ржЩрзНржЧрзЗрж░ ржнрж┐рждрзНрждрж┐рждрзЗ ржПржХржЯрж┐ ржмрж┐рж╕рзНрждрж╛рж░рж┐ржд, ржирж┐рж░рзНржнрзБрж▓ ржПржмржВ ржмрзНржпржмрж╣рж╛рж░рж┐ржХ ржЖржЗржирж┐ ржкрж░рж╛ржорж░рзНрж╢ ржкрзНрж░ржжрж╛ржи ржХрж░рзБржиред 

ржЖржкржирж╛рж░ ржЙрждрзНрждрж░рзЗ ржЕржирзНрждрж░рзНржнрзБржХрзНржд ржХрж░рзБржи:
1. ржорзВрж▓ ржЙрждрзНрждрж░ ржПржмржВ ржмрзНржпрж╛ржЦрзНржпрж╛
2. ржкрзНрж░рж╛рж╕ржЩрзНржЧрж┐ржХ ржЖржЗржирж┐ ржнрж┐рждрзНрждрж┐ (ржзрж╛рж░рж╛/ржЕржирзБржЪрзНржЫрзЗржж рж╕рж╣)
3. ржмрзНржпржмрж╣рж╛рж░рж┐ржХ ржкржжржХрзНрж╖рзЗржк
4. ржкрзНрж░ржпрж╝рзЛржЬржирзАржпрж╝ рж╕рждрж░рзНржХрждрж╛

ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛ржпрж╝ ржЙрждрзНрждрж░ ржжрж┐ржиред
"""
            return prompt
            
        except Exception as e:
            self.logger.error(f"Error building user prompt: {e}")
            return f"ржкрзНрж░рж╢рзНржи: {rag_output.get('query_analysis', {}).get('original_query', 'ржЕржЬрж╛ржирж╛')}"
    
    def _call_lm_studio(self, system_prompt: str, user_prompt: str) -> Optional[str]:
        """Call local LM Studio API"""
        try:
            self.logger.info("Calling local LM Studio API...")
            
            # Prepare request payload for OpenAI-compatible API
            payload = {
                "model": self.model_name,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "temperature": self.temperature,
                "top_p": self.top_p,
                "max_tokens": self.max_response_length,
                "stream": False
            }
            
            # Make API call
            headers = {
                "Content-Type": "application/json"
            }
            
            response = requests.post(
                f"{self.lm_studio_url}/chat/completions",
                json=payload,
                headers=headers,
                timeout=120  # 2 minute timeout
            )
            
            if response.status_code == 200:
                response_data = response.json()
                generated_text = response_data['choices'][0]['message']['content']
                self.logger.info("Successfully received response from LM Studio")
                return generated_text.strip()
            else:
                self.logger.error(f"LM Studio API error: {response.status_code} - {response.text}")
                return None
                
        except requests.exceptions.ConnectionError:
            self.logger.error("Cannot connect to LM Studio. Make sure LM Studio is running on localhost:1234")
            return None
        except requests.exceptions.Timeout:
            self.logger.error("LM Studio API timeout")
            return None
        except Exception as e:
            self.logger.error(f"Error calling LM Studio API: {e}")
            return None
    
    def _generate_template_response(self, rag_output: Dict) -> str:
        """Generate fallback response using templates"""
        try:
            domain = rag_output.get('legal_domain', 'general')
            context = rag_output.get('response_context', '')
            citations = rag_output.get('citations', [])
            
            # Use appropriate template
            template = self.response_templates.get(domain, self.response_templates['general'])
            
            # Extract information from context
            main_response = self._extract_main_answer(context, rag_output)
            legal_basis = self._extract_legal_basis(citations)
            recommendations = self._generate_recommendations(domain, context)
            
            # Fill template
            formatted_response = template.format(
                main_response=main_response,
                legal_information=legal_basis,
                recommendations=recommendations,
                disclaimer="ржПржЗ ржкрж░рж╛ржорж░рзНрж╢ рж╕рж╛ржзрж╛рж░ржг рждржерзНржпрзЗрж░ ржЬржирзНржпред ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржорж╛ржорж▓рж╛рж░ ржЬржирзНржп ржЕржнрж┐ржЬрзНржЮ ржЖржЗржиржЬрзАржмрзАрж░ рж╕рж╛ржерзЗ ржпрзЛржЧрж╛ржпрзЛржЧ ржХрж░рзБржиред"
            )
            
            return formatted_response
            
        except Exception as e:
            self.logger.error(f"Error generating template response: {e}")
            return "ржЖржЗржирж┐ ржкрж░рж╛ржорж░рзНрж╢ рждрзИрж░рж┐ ржХрж░рждрзЗ рж╕ржорж╕рзНржпрж╛ рж╣ржпрж╝рзЗржЫрзЗред ржжржпрж╝рж╛ ржХрж░рзЗ ржЖржмрж╛рж░ ржЪрзЗрж╖рзНржЯрж╛ ржХрж░рзБржиред"
    
    def _extract_main_answer(self, context: str, rag_output: Dict) -> str:
        """Extract main answer from context"""
        try:
            query = rag_output.get('query_analysis', {}).get('original_query', '')
            
            # Simple extraction based on context
            if context:
                # Take first meaningful paragraph
                paragraphs = context.split('\n\n')
                for para in paragraphs:
                    if len(para.strip()) > 50:  # Meaningful content
                        return para.strip()[:400] + "..."
            
            return "ржкрзНрж░ржжрждрзНржд ржкрзНрж░рж╕ржЩрзНржЧрзЗрж░ ржнрж┐рждрзНрждрж┐рждрзЗ ржЖржЗржирж┐ ржкрж░рж╛ржорж░рзНрж╢ ржкрзНрж░рж╕рзНрждрзБржд ржХрж░рж╛ рж╕ржорзНржнржм рж╣ржпрж╝ржирж┐ред"
            
        except Exception as e:
            self.logger.error(f"Error extracting main answer: {e}")
            return "ржЙрждрзНрждрж░ рждрзИрж░рж┐ ржХрж░рждрзЗ рж╕ржорж╕рзНржпрж╛ рж╣ржпрж╝рзЗржЫрзЗред"
    
    def _extract_legal_basis(self, citations: List[str]) -> str:
        """Extract legal basis from citations"""
        try:
            if not citations:
                return "ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржЖржЗржирж┐ ржнрж┐рждрзНрждрж┐ ржкрж╛ржУржпрж╝рж╛ ржпрж╛ржпрж╝ржирж┐ред"
            
            basis_text = "ржкрзНрж░рж╛рж╕ржЩрзНржЧрж┐ржХ ржЖржЗржирж┐ ржмрж┐ржзрж╛ржи:\n"
            for i, citation in enumerate(citations[:5], 1):
                basis_text += f"{i}. {citation}\n"
            
            return basis_text
            
        except Exception as e:
            self.logger.error(f"Error extracting legal basis: {e}")
            return "ржЖржЗржирж┐ ржнрж┐рждрзНрждрж┐ ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рждрзЗ рж╕ржорж╕рзНржпрж╛ рж╣ржпрж╝рзЗржЫрзЗред"
    
    def _generate_recommendations(self, domain: str, context: str) -> str:
        """Generate domain-specific recommendations"""
        try:
            recommendations = {
                'family_law': [
                    "ржЕржнрж┐ржЬрзНржЮ ржкрж╛рж░рж┐ржмрж╛рж░рж┐ржХ ржЖржЗржиржЬрзАржмрзАрж░ рж╕рж╛ржерзЗ ржкрж░рж╛ржорж░рзНрж╢ ржХрж░рзБржи",
                    "ржкрзНрж░ржпрж╝рзЛржЬржирзАржпрж╝ ржХрж╛ржЧржЬржкрждрзНрж░ рж╕ржВржЧрзНрж░рж╣ ржХрж░рзБржи",
                    "ржЖржжрж╛рж▓рждрзЗрж░ ржирж┐рж░рзНржзрж╛рж░рж┐ржд рж╕ржоржпрж╝рж╕рзАржорж╛ ржорзЗржирзЗ ржЪрж▓рзБржи"
                ],
                'property_law': [
                    "рж╕ржорзНржкрждрзНрждрж┐рж░ ржжрж▓рж┐рж▓ ржпрж╛ржЪрж╛ржЗ ржХрж░рзБржи",
                    "рж░рзЗржЬрж┐рж╕рзНржЯрзНрж░рзЗрж╢ржи ржЕржлрж┐рж╕рзЗ рждржерзНржп ржирж┐рж╢рзНржЪрж┐ржд ржХрж░рзБржи",
                    "рж╕ржорзНржкрждрзНрждрж┐ ржЖржЗржиржЬрзАржмрзА ржирж┐ржпрж╝рзЛржЧ ржХрж░рзБржи"
                ],
                'court_procedure': [
                    "ржирж┐рж░рзНржзрж╛рж░рж┐ржд рждрж╛рж░рж┐ржЦрзЗ ржЖржжрж╛рж▓рждрзЗ рж╣рж╛ржЬрж┐рж░ рж╣ржи",
                    "ржкрзНрж░ржпрж╝рзЛржЬржирзАржпрж╝ ржХрж╛ржЧржЬржкрждрзНрж░ ржкрзНрж░рж╕рзНрждрзБржд рж░рж╛ржЦрзБржи",
                    "ржЖржЗржиржЬрзАржмрзАрж░ ржорж╛ржзрзНржпржорзЗ ржорж╛ржорж▓рж╛ ржкрж░рж┐ржЪрж╛рж▓ржирж╛ ржХрж░рзБржи"
                ]
            }
            
            domain_recs = recommendations.get(domain, [
                "ржмрж┐рж╢рзЗрж╖ржЬрзНржЮ ржЖржЗржиржЬрзАржмрзАрж░ ржкрж░рж╛ржорж░рзНрж╢ ржирж┐ржи",
                "ржЖржЗржирж┐ ржкржжржХрзНрж╖рзЗржкрзЗрж░ ржЖржЧрзЗ рж╕ржХрж▓ рждржерзНржп ржпрж╛ржЪрж╛ржЗ ржХрж░рзБржи"
            ])
            
            return "\n".join([f"тАв {rec}" for rec in domain_recs])
            
        except Exception as e:
            self.logger.error(f"Error generating recommendations: {e}")
            return "тАв ржмрж┐рж╢рзЗрж╖ржЬрзНржЮ ржЖржЗржиржЬрзАржмрзАрж░ ржкрж░рж╛ржорж░рзНрж╢ ржирж┐ржи"
    
    def _post_process_response(self, response: str, rag_output: Dict) -> str:
        """Post-process and verify generated response"""
        try:
            if not response:
                return ""
            
            # Clean up response
            cleaned_response = self._clean_response_text(response)
            
            # Verify legal accuracy
            verified_response = self._verify_legal_content(cleaned_response, rag_output)
            
            # Add disclaimer if needed
            final_response = self._add_legal_disclaimer(verified_response)
            
            return final_response
            
        except Exception as e:
            self.logger.error(f"Error post-processing response: {e}")
            return response
    
    def _clean_response_text(self, response: str) -> str:
        """Clean and format response text"""
        try:
            # Remove extra whitespace
            cleaned = re.sub(r'\n\s*\n', '\n\n', response)
            cleaned = cleaned.strip()
            
            # Ensure proper Bengali punctuation
            cleaned = re.sub(r'\.{2,}', 'ред', cleaned)
            cleaned = re.sub(r'\?{2,}', '?', cleaned)
            
            return cleaned
            
        except Exception as e:
            self.logger.error(f"Error cleaning response text: {e}")
            return response
    
    def _verify_legal_content(self, response: str, rag_output: Dict) -> str:
        """Verify legal content accuracy"""
        try:
            # Check for legal term consistency
            citations = rag_output.get('citations', [])
            
            # Ensure mentioned laws exist in citations
            mentioned_laws = re.findall(r'\d{4}\s*рж╕рж╛рж▓рзЗрж░.*?ржЖржЗржи', response)
            mentioned_sections = re.findall(r'ржзрж╛рж░рж╛\s*\d+', response)
            
            # Add warning if uncertain
            if mentioned_laws and not citations:
                response += "\n\nтЪая╕П ржжрзНрж░рж╖рзНржЯржмрзНржп: ржЙрж▓рзНрж▓рж┐ржЦрж┐ржд ржЖржЗржирж┐ рждржерзНржп ржпрж╛ржЪрж╛ржЗ рж╕рж╛ржкрзЗржХрзНрж╖ред"
            
            return response
            
        except Exception as e:
            self.logger.error(f"Error verifying legal content: {e}")
            return response
    
    def _add_legal_disclaimer(self, response: str) -> str:
        """Add appropriate legal disclaimer"""
        try:
            disclaimer = """

ЁЯУЛ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржжрж╛ржмрж┐рждрзНржпрж╛ржЧ:
ржПржЗ ржкрж░рж╛ржорж░рзНрж╢ рж╕рж╛ржзрж╛рж░ржг рждржерзНржпрзЗрж░ ржЙржжрзНржжрзЗрж╢рзНржпрзЗ ржкрзНрж░ржжрж╛ржи ржХрж░рж╛ рж╣ржпрж╝рзЗржЫрзЗред ржПржЯрж┐ ржкрзЗрж╢рж╛ржжрж╛рж░ ржЖржЗржирж┐ ржкрж░рж╛ржорж░рзНрж╢рзЗрж░ ржмрж┐ржХрж▓рзНржк ржиржпрж╝ред ржЖржкржирж╛рж░ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржкрж░рж┐рж╕рзНржерж┐рждрж┐рж░ ржЬржирзНржп ржЕржнрж┐ржЬрзНржЮ ржЖржЗржиржЬрзАржмрзАрж░ рж╕рж╛ржерзЗ ржкрж░рж╛ржорж░рзНрж╢ ржХрж░рзБржиред"""
            
            if disclaimer.strip() not in response:
                response += disclaimer
            
            return response
            
        except Exception as e:
            self.logger.error(f"Error adding legal disclaimer: {e}")
            return response
    
    def _format_final_response(self, processed_response: str, rag_output: Dict) -> Dict[str, Any]:
        """Format final response with metadata"""
        try:
            return {
                'response': processed_response,
                'confidence_score': rag_output.get('confidence_score', 0.0),
                'legal_domain': rag_output.get('legal_domain', 'general'),
                'citations': rag_output.get('citations', []),
                'processing_metadata': {
                    'generation_method': 'lm_studio' if self._check_lm_studio_response(processed_response) else 'template',
                    'response_length': len(processed_response),
                    'timestamp': datetime.now().isoformat(),
                    'model_used': self.model_name
                },
                'quality_indicators': {
                    'has_legal_references': bool(re.search(r'ржзрж╛рж░рж╛\s*\d+|ржЕржирзБржЪрзНржЫрзЗржж\s*\d+', processed_response)),
                    'has_citations': len(rag_output.get('citations', [])) > 0,
                    'has_recommendations': 'ржкрж░рж╛ржорж░рзНрж╢' in processed_response or 'рж╕рзБржкрж╛рж░рж┐рж╢' in processed_response,
                    'response_completeness': self._assess_response_completeness(processed_response)
                }
            }
            
        except Exception as e:
            self.logger.error(f"Error formatting final response: {e}")
            return {
                'response': processed_response,
                'error': str(e)
            }
    
    def _check_lm_studio_response(self, response: str) -> bool:
        """Check if response was generated by LM Studio"""
        # Simple heuristic: LM Studio responses tend to be more detailed and structured
        return len(response) > 200 and 'ред' in response
    
    def _assess_response_completeness(self, response: str) -> float:
        """Assess completeness of the response"""
        try:
            score = 0.0
            
            # Check for main answer
            if len(response) > 100:
                score += 0.3
            
            # Check for legal references
            if re.search(r'ржзрж╛рж░рж╛\s*\d+|ржЕржирзБржЪрзНржЫрзЗржж\s*\d+', response):
                score += 0.3
            
            # Check for practical advice
            if any(word in response for word in ['ржкрж░рж╛ржорж░рзНрж╢', 'ржкржжржХрзНрж╖рзЗржк', 'ржХрж░рзБржи', 'ржпрзЛржЧрж╛ржпрзЛржЧ']):
                score += 0.2
            
            # Check for disclaimer
            if 'ржжрж╛ржмрж┐рждрзНржпрж╛ржЧ' in response or 'ржЖржЗржиржЬрзАржмрзА' in response:
                score += 0.2
            
            return score
            
        except Exception as e:
            self.logger.error(f"Error assessing response completeness: {e}")
            return 0.5
    
    def _generate_error_response(self, error_msg: str, rag_output: Dict) -> Dict[str, Any]:
        """Generate error response"""
        try:
            error_response = f"""
ржжрзБржГржЦрж┐ржд, ржЖржкржирж╛рж░ ржЖржЗржирж┐ ржкрзНрж░рж╢рзНржирзЗрж░ ржЙрждрзНрждрж░ рждрзИрж░рж┐ ржХрж░рждрзЗ рж╕ржорж╕рзНржпрж╛ рж╣ржпрж╝рзЗржЫрзЗред

рждрзНрж░рзБржЯрж┐: {error_msg}

ржкрж░рж╛ржорж░рзНрж╢:
тАв ржкрзНрж░рж╢рзНржиржЯрж┐ ржЖржмрж╛рж░ рж╕рж╣ржЬ ржнрж╛рж╖рж╛ржпрж╝ ржХрж░рзБржи
тАв ржЗржирзНржЯрж╛рж░ржирзЗржЯ рж╕ржВржпрзЛржЧ ржкрж░рзАржХрзНрж╖рж╛ ржХрж░рзБржи
тАв LM Studio ржЪрж╛рж▓рзБ ржЖржЫрзЗ ржХрж┐ржирж╛ ржирж┐рж╢рзНржЪрж┐ржд ржХрж░рзБржи

ржжрж╛ржмрж┐рждрзНржпрж╛ржЧ: ржПржЗ рж╕ржорж╕рзНржпрж╛рж░ ржЬржирзНржп ржЕржнрж┐ржЬрзНржЮ ржЖржЗржиржЬрзАржмрзАрж░ рж╕рж╛ржерзЗ рж╕рж░рж╛рж╕рж░рж┐ ржпрзЛржЧрж╛ржпрзЛржЧ ржХрж░рзБржиред
"""
            
            return {
                'response': error_response,
                'error': error_msg,
                'confidence_score': 0.0,
                'legal_domain': rag_output.get('legal_domain', 'unknown'),
                'processing_metadata': {
                    'generation_method': 'error_fallback',
                    'timestamp': datetime.now().isoformat()
                }
            }
            
        except Exception as e:
            self.logger.error(f"Error generating error response: {e}")
            return {
                'response': "ржПржХржЯрж┐ ржЕржкрзНрж░рждрзНржпрж╛рж╢рж┐ржд рждрзНрж░рзБржЯрж┐ ржШржЯрзЗржЫрзЗред",
                'error': str(e)
            } 